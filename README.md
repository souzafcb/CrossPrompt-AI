# üß† Cross-Prompt IA com RAG e Mem√≥ria

Este projeto implementa uma interface em Streamlit que encadeia m√∫ltiplos modelos LLM usando a estrat√©gia Cross-Prompt com mem√≥ria acumulada e enriquecimento de contexto via RAG (web search).

## Funcionalidades

- Cadeia de racioc√≠nio entre 3 modelos diferentes via LangChain + Ollama
- Mem√≥ria persistente entre execu√ß√µes (JSON)
- Busca na web via DuckDuckGo
- Exporta√ß√£o do resultado em Markdown
- Interface interativa com Streamlit

## Executar localmente

```bash
pip install -r requirements.txt
streamlit run app.py
```

> Certifique-se de ter o [Ollama](https://ollama.com) instalado e os modelos `mistral`, `llama3` e `phi3` prontos.

## Deploy no Streamlit Cloud

1. Suba os arquivos para um reposit√≥rio p√∫blico no GitHub.
2. Acesse [https://streamlit.io/cloud](https://streamlit.io/cloud).
3. Conecte seu reposit√≥rio e selecione `app.py`.
4. Clique em **Deploy** e pronto.
